{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Ask_for_list():\n",
    "    my_list = raw_input('Enter words seperated with space').strip().split(',')\n",
    "    flag = 0\n",
    "    for i in range(0,len(my_list)):\n",
    "        Each_Entry = my_list[i].split()\n",
    "        if len(Each_Entry) > 5:\n",
    "            print('This code works for n-grams n < 6')\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 1:\n",
    "        return Ask_for_list()\n",
    "    else:\n",
    "        return my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One of the documents has Japanese text which is not in ASCII format, so I wanted to remove that part. The following code do it for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "corpus = []\n",
    "path = '/Users/Vahid/Documents/Interview_Materials/BannerTech/2015' \n",
    "for i in os.walk(path).next()[2]:\n",
    "    if i.endswith('1982-0.txt'):\n",
    "        f = open(os.path.join(path,i))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_text = []\n",
    "lines = f.read()\n",
    "f.close()\n",
    "for i in lines.split():\n",
    "    try:\n",
    "        i.decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        None\n",
    "    else:\n",
    "        new_text.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Start Year: 2012\n",
      "Enter End Year: 2012\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "start_year = int(input(\"Enter Start Year: \"))\n",
    "end_year = int(input(\"Enter End Year: \"))\n",
    "for x in range(start_year,end_year+1):\n",
    "    corpus = []\n",
    "    path = '/Users/Vahid/Documents/Interview_Materials/BannerTech/' + str(x)\n",
    "    for i in os.walk(path).next()[2]:\n",
    "        if i.endswith('.txt' or '.utf-8'):\n",
    "            f = open(os.path.join(path,i))\n",
    "            corpus.append(f.read())\n",
    "    frequencies1 = Counter([])\n",
    "    frequencies2 = Counter([])\n",
    "    frequencies3 = Counter([])\n",
    "    frequencies4 = Counter([])\n",
    "    frequencies5 = Counter([])\n",
    "    token_new = []\n",
    "    for text in corpus:\n",
    "        token = nltk.word_tokenize(text)\n",
    "        for word in token:\n",
    "            word = re.sub(\"[\\$\\%\\*\\'\\\"`\\|\\/\\+\\#\\,\\)\\(\\?\\!\\B\\-\\:\\=\\;\\.\\«\\»\\—\\@]\", '', word)\n",
    "            # word = re.findall('\\w+', word)\n",
    "            if word != '':\n",
    "                temp = word.lower()\n",
    "                token_new.append(temp)\n",
    "    \n",
    "    bigrams1 = ngrams(token_new, 1)\n",
    "    bigrams2 = ngrams(token_new, 2)\n",
    "    bigrams3 = ngrams(token_new, 3)\n",
    "    bigrams4 = ngrams(token_new, 4)\n",
    "    bigrams5 = ngrams(token_new, 5)\n",
    "    frequencies1 += Counter(bigrams1)\n",
    "    frequencies2 += Counter(bigrams2)\n",
    "    frequencies3 += Counter(bigrams3)\n",
    "    frequencies4 += Counter(bigrams4)\n",
    "    frequencies5 += Counter(bigrams5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section caculates the occurance rate of the words using n-gram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evil : 0.00108873755437%\n",
      "evel : 0.0%\n",
      "God : 0.0103430067665%\n",
      "sun : 0.0117039287095%\n",
      "son : 0.00626024093762%\n",
      "even : 0.0332064954082%\n"
     ]
    }
   ],
   "source": [
    "my_list = ['evil','evel', 'God','sun','son','even']\n",
    "\n",
    "for i in range(0,len(my_list)):\n",
    "    aa = my_list[i].split()\n",
    "    temp_list = []\n",
    "    for word in aa:\n",
    "        temp_list.append(word.lower())\n",
    "    query = tuple(temp_list)\n",
    "    if len(aa) == 1:\n",
    "        print(str(my_list[i]) + ' : ' + str(frequencies1[query]*100.0/sum(frequencies1.values())) + '%')\n",
    "    elif len(aa) == 2:\n",
    "        print(str(my_list[i]) + ' : ' + str(frequencies2[query]*100.0/sum(frequencies2.values())) + '%')\n",
    "    elif len(aa) == 3:\n",
    "        print(str(my_list[i]) + ' : ' + str(frequencies3[query]*100.0/sum(frequencies3.values())) + '%')\n",
    "    elif len(aa) == 4:\n",
    "        print(str(my_list[i]) + ' : ' + str(frequencies4[query]*100.0/sum(frequencies4.values())) + '%')\n",
    "    elif len(aa) == 5:\n",
    "        print(str(my_list[i]) + ' : ' + str(frequencies5[query]*100.0/sum(frequencies5.values())) + '%')\n",
    "    else:\n",
    "        print(str(my_list[i]) + ' : ' + 'The ngrams for n > 5 is not supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ex. : my_list = ['evil','evel', 'God','sun','son','even']\n",
    "def score_calc(my_list):\n",
    "    score_list = []\n",
    "    for i in range(0,len(my_list)):\n",
    "        Each_Entry = my_list[i].split()\n",
    "        temp_list = []\n",
    "        for word in Each_Entry:\n",
    "            temp_list.append(word.lower())\n",
    "        query = tuple(temp_list)\n",
    "        if len(Each_Entry) == 1:\n",
    "            score_list.append(frequencies1[query]*100.0/sum(frequencies1.values()))\n",
    "        elif len(Each_Entry) == 2:\n",
    "            score_list.append(frequencies2[query]*100.0/sum(frequencies2.values()))\n",
    "        elif len(Each_Entry) == 3:\n",
    "            score_list.append(frequencies3[query]*100.0/sum(frequencies3.values()))\n",
    "        elif len(Each_Entry) == 4:\n",
    "            score_list.append(frequencies4[query]*100.0/sum(frequencies4.values()))\n",
    "        elif len(Each_Entry) == 5:\n",
    "            score_list.append(frequencies5[query]*100.0/sum(frequencies5.values()))\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0010887375543688317,\n",
       " 0.0,\n",
       " 0.0103430067665039,\n",
       " 0.01170392870946494,\n",
       " 0.006260240937620782,\n",
       " 0.03320649540824937]"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = score_calc(['evil','evel', 'God','sun','son','even'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section suggests a correction for a word by checking the unigram (it suggests a word with higher probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'even'"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "    s = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [a + b[1:] for a, b in s if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in s if len(b)>1]\n",
    "    replaces   = [a + c + b[1:] for a, b in s for c in alphabet if b]\n",
    "    inserts    = [a + c + b     for a, b in s for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known(words):\n",
    "    return set(w for w in words if frequencies1[tuple([w])] > 0)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known(edits1(word)) or [word]\n",
    "    d = {}\n",
    "    for i in list(candidates):\n",
    "        d[i] = frequencies1[tuple([i])]\n",
    "    return max(d, key=d.get)\n",
    "\n",
    "\n",
    "# Run \"correct\" function to give you a correct choice\n",
    "\n",
    "# The example for 'evel'. It gives the 'even'\n",
    "correct('evel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Inputs are:\n",
    "# start_year, end_year, my_list (list of entered sequence of words), score_list (for each year)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:my_projects_env]",
   "language": "python",
   "name": "conda-env-my_projects_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
